{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZrJT5IlmxuYco6tHFS4kI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/facebookresearch/dinov2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkfmKYtFhiMw","executionInfo":{"status":"ok","timestamp":1699356227459,"user_tz":-330,"elapsed":1756,"user":{"displayName":"Thesis","userId":"01522206886555927655"}},"outputId":"6e26e736-5306-48c5-b8b4-084ef4160f05"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'dinov2'...\n","remote: Enumerating objects: 474, done.\u001b[K\n","remote: Counting objects: 100% (317/317), done.\u001b[K\n","remote: Compressing objects: 100% (168/168), done.\u001b[K\n","remote: Total 474 (delta 197), reused 178 (delta 147), pack-reused 157\u001b[K\n","Receiving objects: 100% (474/474), 1.47 MiB | 4.89 MiB/s, done.\n","Resolving deltas: 100% (238/238), done.\n"]}]},{"cell_type":"code","source":["%cd dinov2/dinov2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvpdMp5Fh28O","executionInfo":{"status":"ok","timestamp":1699361891652,"user_tz":-330,"elapsed":597,"user":{"displayName":"Thesis","userId":"01522206886555927655"}},"outputId":"d38a787c-d330-46bb-8f46-1d581c6c1974"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dinov2/dinov2\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-V7UspYxPbT","executionInfo":{"status":"ok","timestamp":1699361892551,"user_tz":-330,"elapsed":9,"user":{"displayName":"Thesis","userId":"01522206886555927655"}},"outputId":"29ef800f-87fc-47c9-b451-c246873541d2"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["configs  distributed  fsdp  __init__.py  logging  models  train\n","data\t eval\t      hub   layers\t loss\t  run\t  utils\n"]}]},{"cell_type":"markdown","source":["Random resized augmentation:Data Augmentation Pipelines:\n","\n","Three data augmentation pipelines are defined using the previously defined transformations: global_transfo1, global_transfo2, and local_transfo. Each pipeline combines geometric augmentations, color distortions, blurring, and normalization.\n","__call__ Method:\n","\n","The __call__ method takes an image as input and applies data augmentation.\n","It first performs global crops by applying geometric augmentations and the defined data augmentation pipelines (global_transfo1 and global_transfo2) to the input image.\n","It also generates local crops by repeating the process self.local_crops_number times and storing the results in the local_crops list.\n","The augmented data is returned as a dictionary named output. The dictionary contains entries for global crops, global crops for the teacher, local crops, and offsets (empty tuple).\n","This code is intended for data augmentation in the context of a machine learning or deep learning pipeline, where the data is transformed and prepared for training or evaluation of computer vision models.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"r-kUC8iK9OZY"}},{"cell_type":"code","execution_count":45,"metadata":{"id":"8MFLIfE2gyg0","executionInfo":{"status":"ok","timestamp":1699362210444,"user_tz":-330,"elapsed":601,"user":{"displayName":"Thesis","userId":"01522206886555927655"}}},"outputs":[],"source":["\n","\n","import logging\n","\n","from torchvision import transforms\n","from data.transforms import (\n","    GaussianBlur,\n","    make_normalize_transform,\n",")\n","\n","\n","logger = logging.getLogger(\"dinov2\")\n","\n","\n","class DataAugmentationDINO(object):\n","    def __init__(\n","        self,\n","        global_crops_scale,\n","        local_crops_scale,\n","        local_crops_number,\n","        global_crops_size=224,\n","        local_crops_size=96,\n","    ):\n","        self.global_crops_scale = global_crops_scale\n","        self.local_crops_scale = local_crops_scale\n","        self.local_crops_number = local_crops_number\n","        self.global_crops_size = global_crops_size\n","        self.local_crops_size = local_crops_size\n","\n","        logger.info(\"###################################\")\n","        logger.info(\"Using data augmentation parameters:\")\n","        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n","        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n","        logger.info(f\"local_crops_number: {local_crops_number}\")\n","        logger.info(f\"global_crops_size: {global_crops_size}\")\n","        logger.info(f\"local_crops_size: {local_crops_size}\")\n","        logger.info(\"###################################\")\n","\n","        # random resized crop and flip\n","        self.geometric_augmentation_global = transforms.Compose(\n","            [\n","                transforms.RandomResizedCrop(\n","                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n","                ),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","            ]\n","        )\n","\n","        self.geometric_augmentation_local = transforms.Compose(\n","            [\n","                transforms.RandomResizedCrop(\n","                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n","                ),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","            ]\n","        )\n","\n","        # color distorsions / blurring\n","        color_jittering = transforms.Compose(\n","            [\n","                transforms.RandomApply(\n","                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n","                    p=0.8,\n","                ),\n","                transforms.RandomGrayscale(p=0.2),\n","            ]\n","        )\n","\n","        global_transfo1_extra = GaussianBlur(p=1.0)\n","\n","        global_transfo2_extra = transforms.Compose(\n","            [\n","                GaussianBlur(p=0.1),\n","                transforms.RandomSolarize(threshold=128, p=0.2),\n","            ]\n","        )\n","\n","        local_transfo_extra = GaussianBlur(p=0.5)\n","\n","        # normalization\n","        self.normalize = transforms.Compose(\n","            [\n","                transforms.ToTensor(),\n","                make_normalize_transform(),\n","            ]\n","        )\n","\n","        self.global_transfo1 = transforms.Compose([color_jittering, global_transfo1_extra, self.normalize])\n","        self.global_transfo2 = transforms.Compose([color_jittering, global_transfo2_extra, self.normalize])\n","        self.local_transfo = transforms.Compose([color_jittering, local_transfo_extra, self.normalize])\n","\n","    def __call__(self, image):\n","        output = {}\n","\n","        # global crops:\n","        im1_base = self.geometric_augmentation_global(image)\n","        global_crop_1 = self.global_transfo1(im1_base)\n","\n","        im2_base = self.geometric_augmentation_global(image)\n","        global_crop_2 = self.global_transfo2(im2_base)\n","\n","        output[\"global_crops\"] = [global_crop_1, global_crop_2]\n","\n","        # global crops for teacher:\n","        output[\"global_crops_teacher\"] = [global_crop_1, global_crop_2]\n","\n","        # local crops:\n","        local_crops = [\n","            self.local_transfo(self.geometric_augmentation_local(image)) for _ in range(self.local_crops_number)\n","        ]\n","        output[\"local_crops\"] = local_crops\n","        output[\"offsets\"] = ()\n","\n","        return output"]}]}