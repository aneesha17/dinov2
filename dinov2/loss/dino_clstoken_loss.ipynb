{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2tTGESojL7j5dJ0DaWo1f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["•Patch-level objective (Zhou et al., 2021). We randomly mask some of the input patches given\n","to the student, but not to the teacher. We then add a cross-entropy loss between the patch features\n","of both networks on each masked patch. This loss is combined with the image-level loss.\n","\n","• Untying head weights between both objectives. We observe that tying the weights associated\n","with both objectives makes the model underfit at the patch-level while overfitting at the image-level.\n","Untying these weights resolves this issue and improve the performances at both scales.\n","\n","• Sinkhorn-Knopp centering (Caron et al., 2020). Ruan et al. (2022) recommend to replace the\n","teacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization\n","of SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the\n","student, we apply the softmax normalization."],"metadata":{"id":"W0RUXnc_QwPR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Js8qvxFkQo0r"},"outputs":[],"source":["\n","import torch\n","import torch.distributed as dist\n","import torch.nn.functional as F\n","from torch import nn\n","\n","\n","class DINOLoss(nn.Module):\n","    def __init__(\n","        self,\n","        out_dim,\n","        student_temp=0.1,\n","        center_momentum=0.9,\n","    ):\n","        super().__init__()\n","        self.student_temp = student_temp\n","        self.center_momentum = center_momentum\n","        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n","        self.updated = True\n","        self.reduce_handle = None\n","        self.len_teacher_output = None\n","        self.async_batch_center = None\n","\n","    @torch.no_grad()\n","    def softmax_center_teacher(self, teacher_output, teacher_temp):\n","        self.apply_center_update()\n","        # teacher centering and sharpening\n","        return F.softmax((teacher_output - self.center) / teacher_temp, dim=-1)\n","\n","    @torch.no_grad()\n","    def sinkhorn_knopp_teacher(self, teacher_output, teacher_temp, n_iterations=3):\n","        teacher_output = teacher_output.float()\n","        world_size = dist.get_world_size() if dist.is_initialized() else 1\n","        Q = torch.exp(teacher_output / teacher_temp).t()  # Q is K-by-B for consistency with notations from our paper\n","        B = Q.shape[1] * world_size  # number of samples to assign\n","        K = Q.shape[0]  # how many prototypes\n","\n","        # make the matrix sums to 1\n","        sum_Q = torch.sum(Q)\n","        if dist.is_initialized():\n","            dist.all_reduce(sum_Q)\n","        Q /= sum_Q\n","\n","        for it in range(n_iterations):\n","            # normalize each row: total weight per prototype must be 1/K\n","            sum_of_rows = torch.sum(Q, dim=1, keepdim=True)\n","            if dist.is_initialized():\n","                dist.all_reduce(sum_of_rows)\n","            Q /= sum_of_rows\n","            Q /= K\n","\n","            # normalize each column: total weight per sample must be 1/B\n","            Q /= torch.sum(Q, dim=0, keepdim=True)\n","            Q /= B\n","\n","        Q *= B  # the columns must sum to 1 so that Q is an assignment\n","        return Q.t()\n","\n","    def forward(self, student_output_list, teacher_out_softmaxed_centered_list):\n","        \"\"\"\n","        Cross-entropy between softmax outputs of the teacher and student networks.\n","        \"\"\"\n","        # TODO: Use cross_entropy_distribution here\n","        total_loss = 0\n","        for s in student_output_list:\n","            lsm = F.log_softmax(s / self.student_temp, dim=-1)\n","            for t in teacher_out_softmaxed_centered_list:\n","                loss = torch.sum(t * lsm, dim=-1)\n","                total_loss -= loss.mean()\n","        return total_loss\n","\n","    @torch.no_grad()\n","    def update_center(self, teacher_output):\n","        self.reduce_center_update(teacher_output)\n","\n","    @torch.no_grad()\n","    def reduce_center_update(self, teacher_output):\n","        self.updated = False\n","        self.len_teacher_output = len(teacher_output)\n","        self.async_batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n","        if dist.is_initialized():\n","            self.reduce_handle = dist.all_reduce(self.async_batch_center, async_op=True)\n","\n","    @torch.no_grad()\n","    def apply_center_update(self):\n","        if self.updated is False:\n","            world_size = dist.get_world_size() if dist.is_initialized() else 1\n","\n","            if self.reduce_handle is not None:\n","                self.reduce_handle.wait()\n","            _t = self.async_batch_center / (self.len_teacher_output * world_size)\n","\n","            self.center = self.center * self.center_momentum + _t * (1 - self.center_momentum)\n","\n","            self.updated = True"]}]}